# MARL è®­ç»ƒæ•°æ®è®°å½•æŒ‡å—

## ğŸ“Š æ¦‚è¿°

ä½ å·²åœ¨ `MARL_training.py` ä¸­å®ç°äº†åŸºäºæœ€ä¼˜è¶…å‚æ•°ç»„åˆè®­ç»ƒçš„æ¨¡å‹çš„è®­ç»ƒæ•°æ®è®°å½•åŠŸèƒ½ã€‚

## ğŸ¯ è®°å½•çš„å†…å®¹

### 1. æœ€ä¼˜è¶…å‚æ•°è®°å½•
**æ–‡ä»¶**: `final_training_logs/optimal_hyperparams.txt`

è®°å½•æ‰€æœ‰æœ€ä¼˜çš„ reward æƒé‡é…ç½®ï¼š
```
=== æœ€ä¼˜è¶…å‚æ•°ç»„åˆ ===
w_pos_err: 0.4
w_proprio_err: 0.4
w_activation: 0.4
...
```

### 2. è®­ç»ƒæŒ‡æ ‡è®°å½•
**æ–‡ä»¶**: `final_training_logs/training_metrics.csv`

æ¯ä¸ªè®­ç»ƒ iteration è®°å½•ä»¥ä¸‹æŒ‡æ ‡ï¼š
- `iteration`: è¿­ä»£æ¬¡æ•°
- `exo_reward`: å¤–éª¨éª¼ç­–ç•¥å¥–åŠ±
- `human_reward`: äººç±»ç­–ç•¥å¥–åŠ±
- `episode_return_mean`: å¹³å‡å›åˆå›æŠ¥
- `episode_len_mean`: å¹³å‡å›åˆé•¿åº¦
- `policy_loss_human`: äººç±»ç­–ç•¥æŸå¤±
- `policy_loss_exo`: å¤–éª¨éª¼ç­–ç•¥æŸå¤±
- `vf_loss_human`: äººç±»ä»·å€¼å‡½æ•°æŸå¤±
- `vf_loss_exo`: å¤–éª¨éª¼ä»·å€¼å‡½æ•°æŸå¤±
- `entropy_human`: äººç±»ç­–ç•¥ç†µ
- `entropy_exo`: å¤–éª¨éª¼ç­–ç•¥ç†µ
- `num_episodes`: æœ¬è¿­ä»£ç”Ÿæˆçš„å›åˆæ•°

### 3. Episode çº§å¥–åŠ±ç»†èŠ‚
**æ–‡ä»¶**: `reward_terms_by_episode.csv`

ï¼ˆæ¥è‡ª `RewardLoggingCallbacks`ï¼‰è®°å½•æ¯ä¸ª episode çš„è¯¦ç»†å¥–åŠ±åˆ†é‡ï¼š
- Human: R_pï¼ˆä½ç½®é”™è¯¯ï¼‰, R_proprioï¼ˆæœ¬ä½“æ„Ÿè§‰ï¼‰ï¼ŒR_aï¼ˆæ¿€æ´»ï¼‰
- Exo: R_tï¼ˆæ‰­çŸ©ï¼‰ï¼ŒR_easï¼ˆå¹³æ»‘æ€§ï¼‰

## ğŸ“ è¾“å‡ºç›®å½•ç»“æ„

```
.
â”œâ”€â”€ final_training_logs/                      # æœ¬æ¬¡æœ€ä¼˜è¶…å‚è®­ç»ƒçš„æ—¥å¿—
â”‚   â”œâ”€â”€ optimal_hyperparams.txt              # æœ€ä¼˜è¶…å‚æ•°
â”‚   â””â”€â”€ training_metrics.csv                 # è®­ç»ƒæŒ‡æ ‡
â”œâ”€â”€ final_policy_checkpoints/                # å®šæœŸä¿å­˜çš„æ£€æŸ¥ç‚¹
â”œâ”€â”€ best_policy/                             # å†å²æœ€ä¼˜æ¨¡å‹
â”œâ”€â”€ best_policy/checkpoint_*/                # å…·ä½“æ£€æŸ¥ç‚¹æ–‡ä»¶
â”œâ”€â”€ reward_terms_by_episode.csv              # Episode çº§å¥–åŠ±ç»†èŠ‚
â””â”€â”€ ray_results/                             # ç½‘æ ¼æœç´¢ç»“æœ
    â””â”€â”€ marl_exo_reward_grid/
        â””â”€â”€ PPO_MARL_EXO_ENV_*/
            â”œâ”€â”€ progress.csv                 # ç½‘æ ¼æœç´¢æ¯ä¸ªè¯•éªŒçš„è¿›åº¦
            â””â”€â”€ checkpoint_*/                # æ¯ä¸ªè¯•éªŒçš„æ£€æŸ¥ç‚¹
```

## ğŸ”„ å·¥ä½œæµç¨‹

### ç¬¬ä¸€é˜¶æ®µï¼šç½‘æ ¼æœç´¢
```python
best = param_search(policies)
# è¿è¡Œæ‰€æœ‰è¶…å‚æ•°ç»„åˆï¼Œæ¯ä¸ª 7 iteration
# é€‰æ‹©æœ€ä¼˜çš„ç»„åˆ
```
è¾“å‡ºï¼š
- æ‰€æœ‰è¶…å‚æ•°ç»„åˆçš„è®­ç»ƒç»“æœåœ¨ `ray_results/marl_exo_reward_grid/`
- æ¯ä¸ªè¯•éªŒçš„ `progress.csv` è®°å½•äº†å„è‡ªçš„è®­ç»ƒæŒ‡æ ‡

### ç¬¬äºŒé˜¶æ®µï¼šæœ€ä¼˜è¶…å‚æ•°è®­ç»ƒ
```python
final_config = PPOConfig().update_from_dict(best.config)
algo = final_config.build()
algo.restore(best.checkpoint.path)
# ç»§ç»­è®­ç»ƒ 6 iteration
```
è¾“å‡ºï¼š
- `optimal_hyperparams.txt`: æœ€ä¼˜è¶…å‚æ•°
- `training_metrics.csv`: è¯¦ç»†çš„è®­ç»ƒæŒ‡æ ‡
- `best_policy/`: æœ€å¥½çš„æ¨¡å‹
- `final_policy_checkpoints/`: å®šæœŸä¿å­˜çš„æ£€æŸ¥ç‚¹

## ğŸ“ˆ å¦‚ä½•ä½¿ç”¨è¿™äº›æ•°æ®

### åˆ†æè®­ç»ƒæ›²çº¿
```python
import pandas as pd
import matplotlib.pyplot as plt

# è¯»å–æœ€ä¼˜è¶…å‚æ•°è®­ç»ƒçš„æŒ‡æ ‡
df = pd.read_csv('final_training_logs/training_metrics.csv')

# ç»˜åˆ¶å¥–åŠ±æ›²çº¿
plt.figure(figsize=(12, 4))

plt.subplot(1, 3, 1)
plt.plot(df['iteration'], df['exo_reward'], label='Exo')
plt.plot(df['iteration'], df['human_reward'], label='Human')
plt.xlabel('Iteration')
plt.ylabel('Reward')
plt.legend()
plt.title('Policy Returns')

plt.subplot(1, 3, 2)
plt.plot(df['iteration'], df['policy_loss_human'], label='Human')
plt.plot(df['iteration'], df['policy_loss_exo'], label='Exo')
plt.xlabel('Iteration')
plt.ylabel('Policy Loss')
plt.legend()
plt.title('Policy Loss')

plt.subplot(1, 3, 3)
plt.plot(df['iteration'], df['entropy_human'], label='Human')
plt.plot(df['iteration'], df['entropy_exo'], label='Exo')
plt.xlabel('Iteration')
plt.ylabel('Entropy')
plt.legend()
plt.title('Policy Entropy')

plt.tight_layout()
plt.savefig('final_training_logs/training_analysis.png', dpi=150)
plt.show()
```

### å¯¹æ¯”ç½‘æ ¼æœç´¢å’Œæœ€ä¼˜è¶…å‚æ•°è®­ç»ƒ
```python
# è¯»å–ç½‘æ ¼æœç´¢ç»“æœ
grid_results = pd.read_csv('ray_results/marl_exo_reward_grid/PPO_MARL_EXO_ENV_*/progress.csv')

# è¯»å–æœ€ä¼˜è¶…å‚æ•°è®­ç»ƒ
optimal_training = pd.read_csv('final_training_logs/training_metrics.csv')

# æ¯”è¾ƒæœ€ç»ˆæ€§èƒ½
print(f"ç½‘æ ¼æœç´¢æœ€ä¼˜: {grid_results['episode_return_mean'].max()}")
print(f"æœ€ä¼˜è¶…å‚æ•°ç»§ç»­è®­ç»ƒ: {optimal_training['episode_return_mean'].iloc[-1]}")
```

## ğŸš€ ä¸»è¦æ”¹è¿›åŠŸèƒ½

1. **è‡ªåŠ¨åŒ–æ—¥å¿—è®°å½•**ï¼šæ— éœ€æ‰‹åŠ¨æ”¶é›†æ•°æ®
2. **ç»“æ„åŒ–å­˜å‚¨**ï¼šä½¿ç”¨ CSV æ ¼å¼ä¾¿äºåç»­åˆ†æ
3. **å®Œæ•´ä¿¡æ¯ä¿å­˜**ï¼šä»è¶…å‚æ•°åˆ°è®­ç»ƒæŒ‡æ ‡å…¨è¦†ç›–
4. **å¤šå±‚æ¬¡æ•°æ®**ï¼šç½‘æ ¼æœç´¢ â†’ æœ€ä¼˜è¶…å‚ â†’ Episode ç»†èŠ‚

## âš™ï¸ é…ç½®è°ƒæ•´

å¦‚æœéœ€è¦ä¿®æ”¹æ—¥å¿—è¡Œä¸ºï¼š

```python
# åœ¨ main() å‡½æ•°ä¸­ä¿®æ”¹
log_dir = "final_training_logs"  # ä¿®æ”¹æ—¥å¿—ç›®å½•
metrics_csv_file = os.path.join(abs_log_dir, "training_metrics.csv")

# åœ¨ _init_metrics_csv() ä¸­æ·»åŠ æ›´å¤šå­—æ®µ
writer.writerow([
    "iteration",
    "exo_reward",
    # æ·»åŠ å…¶ä»–ä½ éœ€è¦çš„æŒ‡æ ‡...
])
```

## ğŸ“ ç¤ºä¾‹è¾“å‡º

### optimal_hyperparams.txt
```
=== æœ€ä¼˜è¶…å‚æ•°ç»„åˆ ===
w_pos_err: 0.4
w_proprio_err: 0.4
w_activation: 0.4
w_exo_energy: 0.2
w_exo_smooth: 0.2
theta_pos_err: 0.5
theta_proprio_err: 0.3
theta_activation: 0.1
theta_exo_energy: 0.1
theta_exo_smooth: 4
```

### training_metrics.csv ç¤ºä¾‹
```
iteration,exo_reward,human_reward,episode_return_mean,episode_len_mean,...
0,-150.32,-120.45,-270.77,250.5,...
1,-145.20,-118.60,-263.80,251.2,...
2,-142.15,-115.30,-257.45,252.1,...
...
```

## ğŸ”§ æ•…éšœæ’é™¤

**æ—¥å¿—æ–‡ä»¶æ²¡æœ‰è¢«åˆ›å»ºï¼Ÿ**
- æ£€æŸ¥ `final_training_logs/` ç›®å½•æ˜¯å¦æœ‰å†™å…¥æƒé™
- ç¡®ä¿ä½ çš„ä»£ç æˆåŠŸå®Œæˆäº†ç½‘æ ¼æœç´¢é˜¶æ®µ

**CSV æ–‡ä»¶ä¸ºç©ºï¼Ÿ**
- æ£€æŸ¥ `algo.train()` æ˜¯å¦æ­£å¸¸å·¥ä½œ
- éªŒè¯ result å­—å…¸çš„ç»“æ„æ˜¯å¦ä¸é¢„æœŸåŒ¹é…

**ç¼ºå°‘æŸäº›å­—æ®µï¼Ÿ**
- åœ¨ `_log_training_metrics()` ä¸­æ·»åŠ ç›¸åº”çš„å­—æ®µæå–é€»è¾‘
- åŒæ—¶åœ¨ `_init_metrics_csv()` ä¸­çš„è¡¨å¤´ä¸­æ·»åŠ å¯¹åº”åˆ—

---

**æ›´æ–°æ—¶é—´**: 2025å¹´11æœˆ20æ—¥  
**ç›¸å…³æ–‡ä»¶**: `MARL_training.py`, `Custom_CallBack.py`
